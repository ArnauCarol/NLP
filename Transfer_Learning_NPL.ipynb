{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer learning:** pre-trained text embeddings\n",
        "\n",
        "For this example we will use a pre-trained text embedding model from TensorFlow Hub called **google/nnlm-en-dim128/2**.\n",
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "*   We don't have to worry about text preprocessing.\n",
        "*   We can benefit from transfer learning.\n",
        "*   The embedding has a fixed size, so it's simpler to process.\n",
        "\n",
        "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences."
      ],
      "metadata": {
        "id": "_fkSmrOjaeTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-13T03:51:08.377448Z",
          "iopub.status.busy": "2021-01-13T03:51:08.376763Z",
          "iopub.status.idle": "2021-01-13T03:51:15.989069Z",
          "shell.execute_reply": "2021-01-13T03:51:15.988434Z"
        },
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Loading IMBD Dataset\n",
        "\n",
        "\n",
        "We’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The parameter num_words controls how many words different we want to use.\n",
        "\n",
        "\n",
        "We are going to download the dataset using [TFDS](https://www.tensorflow.org/datasets). TFDS provides a collection of ready-to-use datasets for use with TensorFlow, Jax, and other Machine Learning frameworks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "PN51LEh8za1v"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-01-13T03:51:16.001184Z",
          "iopub.status.busy": "2021-01-13T03:51:16.000514Z",
          "iopub.status.idle": "2021-01-13T03:51:17.568416Z",
          "shell.execute_reply": "2021-01-13T03:51:17.568875Z"
        },
        "id": "SHRwRoP2nVHX",
        "outputId": "18300537-4aa3-4db4-ee8c-37cd35f1e8e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-01-13T03:51:17.574503Z",
          "iopub.status.busy": "2021-01-13T03:51:17.573198Z",
          "iopub.status.idle": "2021-01-13T03:51:17.618857Z",
          "shell.execute_reply": "2021-01-13T03:51:17.619346Z"
        },
        "id": "vd4_BGKyurao",
        "outputId": "2571207d-32af-4de6-e3dc-c4070e74472c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "label:  0\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print('text: ', example.numpy())\n",
        "    print('--'*100)\n",
        "    print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-13T03:51:17.623697Z",
          "iopub.status.busy": "2021-01-13T03:51:17.623008Z",
          "iopub.status.idle": "2021-01-13T03:51:17.624931Z",
          "shell.execute_reply": "2021-01-13T03:51:17.625389Z"
        },
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-13T03:51:17.629810Z",
          "iopub.status.busy": "2021-01-13T03:51:17.629159Z",
          "iopub.status.idle": "2021-01-13T03:51:17.632829Z",
          "shell.execute_reply": "2021-01-13T03:51:17.633249Z"
        },
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-01-13T03:51:17.638666Z",
          "iopub.status.busy": "2021-01-13T03:51:17.637321Z",
          "iopub.status.idle": "2021-01-13T03:51:17.883181Z",
          "shell.execute_reply": "2021-01-13T03:51:17.883586Z"
        },
        "id": "jqkvdcFv41wC",
        "outputId": "71eca594-7a67-4f71-b8bc-97280259ab95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'Erich Rohmer\\'s \"L\\'Anglaise et le duc\" makes a perfect companion piece to Peter Watkins\\' \"La Commune (Paris 1871).\" Both films -screened at this year\\'s Toronto International Film Festival- ironically illustrate how history is shaped to by the tellers of the tale. Ironic, given the tragic events that were taking place in the U.S. during the festival.<br /><br />Set in Paris during the French Revolution, the movie, based on Grace Elliott\\'s (Lucy Russell) \"Memoirs,\" is a first-hand account of how she survived those heady but dangerous days. She also details her relationship with The Duke of Orleans (played by Jean-Claude Dreyfus), who, in contrast to herself, is a supporter of the Revolution.<br /><br />True to form, you don\\'t know whose side of history Rohmer is going to come down on. One of the earliest of the French \"New Wave\" filmmakers, Rohmer has often been criticized for being too conservative. After all, in the midst of the rebelling-youth-Viet-Nam days of the late 60s and 70s, he was filming romantic little confections like \"Claire\\'s Knee.\" But don\\'t sell the old boy short, folks, he\\'s always been a student of human nature, not an ideologue, and \"L\\'Anglaise et le duc\" continues to bear this out.<br /><br />Rohmer\\'s characters are never the \"bad guys\" nor the \"good guys\\'; they are first and foremost human beings who are capable of exhibiting a full range of human potentialities -and limitations. That\\'s why his movies are always provocative, and this film is no exception.<br /><br />Now for the technological nuts and bolts.<br /><br />Rohmer, though making his way into his 80s, is still on the cutting-edge of cinematic innovation. The look of \"L\\'Anglaise\" is like something you\\'ve never seen before. You guessed it, the old guy -like several of the festival\\'s directors this year- has gone digital.<br /><br />All of the movie\\'s exterior scenes look as though they are taking place in their original 1780s Parisian settings. As a matter of fact, you may get so distracted from marveling at the authenticity of the film\\'s look you may have to go back for a second screening to catch the subtleties of the film\\'s psychological -and yes, I\\'ll say it- political insights.<br /><br />Toronto features some of the world\\'s edgiest young filmmakers this year, as well as some of the world\\'s oldest. And the old masters are standing there on cinema\\'s cutting-edges right alongside the young ones.<br /><br />Long live youth. Long live old age. And long live Erich Rohmer.<br /><br />'\n",
            " b'Rajinikanth becomes born again after getting a magical power which he can use seven times.<br /><br />There are several problems with this movie that are obvious to the casual audience: the 50ish Rajinikanth is still at home with his parents; the father of the girl next door thinks that he is a compelling \"boy\" (\\'vaseekaramaana paiyan\\'); Rajinikanth suddenly interrupts the movie with his sermons, the worst being how women of yesteryears got their exercise through household work--yet we are to believe that he is not a theist; even though he was well read, he wastes six of his seven powers on a stupid kite; I can go on, but you get the picture.<br /><br />There are god-men, there are gods, and there is Rajinikanth. The directory has difficulty fitting Rajinikanth into one of these categories. Initially, Rajinikanth is just Rajinikanth doing what Tamil heroes do--stand up to villains and, in spite of being the oldest, getting courted by the prettiest girl in the movie. Rajinikanth does this well and some of Rajinikanth\\'s trademark styles are actually enjoyable--\"baba count\" is a novelty. What makes this movie unbearable is that those few initial minutes are just a preface to an worst book to be ever written. Even that preface is punctuated with some comedy which are forced and obvious.<br /><br />The director doesn\\'t explain the purpose of the hero; we see that the hero is facing several hurdles (from politicians, as usual) but we can\\'t really root for the hero because we don\\'t know what the hero\\'s ultimate goal is. At the end, when everyone wants him to be the leader, the hero gives another one of his sermons and walks away to become a hermit. The director offers no solution to the problem in the climax scene.<br /><br />A. R. Rehman\\'s score is really interesting. Either he shows patches of brilliance or he didn\\'t bother to invest himself fully into this movie--who can blame him. There is one scene where Rajinikanth steps into the van of one of the crooks and then throws the knife and starts his baba count. The music is very apt for the moment and acts as a catalyst adding further tension. The songs are all mediocre, no one would bother with the songs from this movie after a few years.<br /><br />Unfortunately, 1 is the lowest rank you can assign in IMDb. This movie has all the elements that justify its rightful place at the nether of IMDb\\'s ranking.'\n",
            " b\"Carlos Saura's Carmen is one of the finest achievements in world, let alone Spanish, cinema. It manages to excite interest in flamenco in its wonderful staged adaptations from Bizet with powerful physical force. At the same time we see the impact of the creation and rehearsal of a new interpretation of Carmen on the choreographer/director and the principle dancers. The fine line between life and art is dazzling.\"]\n",
            "\n",
            "labels:  [1 0 1]\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print('texts: ', example.numpy()[:3])\n",
        "    print()\n",
        "    print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-13T03:57:34.284326Z",
          "iopub.status.busy": "2021-01-13T03:57:34.283544Z",
          "iopub.status.idle": "2021-01-13T03:57:34.574582Z",
          "shell.execute_reply": "2021-01-13T03:57:34.575021Z"
        },
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [],
      "source": [
        "def show_loss_accuracy_evolution(history):\n",
        "    \n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Sparse Categorical Crossentropy')\n",
        "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
        "    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
        "    ax2.grid()\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haf49rcoza16"
      },
      "source": [
        "### Transfer learning: pre-trained text embeddings\n",
        "\n",
        "For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://tfhub.dev) called [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2).\n",
        "\n",
        "[TensorFlow Hub](https://tfhub.dev/) has hundreds of trained, ready-to-deploy machine learning models.  You can find more [text embedding models](https://tfhub.dev/s?module-type=text-embedding) on TFHub.\n",
        "\n",
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "*   we don't have to worry about text preprocessing,\n",
        "*   we can benefit from transfer learning,\n",
        "*   the embedding has a fixed size, so it's simpler to process.\n",
        "\n",
        "For this example we will use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim128/2.\n",
        "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "GwYXxtZzza16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5439780c-a273-4b78-e6dc-cbd75e91f982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "lEVBJ1G9za16"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "r_O_JeOeza16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64551d1-6b59-407b-ccdb-c3816ee681f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_2 (KerasLayer)  (None, 128)               124642688 \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 25)                3225      \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 26        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 124,645,939\n",
            "Trainable params: 124,645,939\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(25, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "hNlBKj1Zza16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55481da-97b8-4542-bc5a-9fee0991dd8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 17s 84ms/step - loss: 0.4781 - accuracy: 0.7918 - val_loss: 0.3729 - val_accuracy: 0.8438\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='BinaryCrossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_dataset, epochs=1,\n",
        "                    validation_data=test_dataset, \n",
        "                    validation_steps=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "V5SY_hCvn0qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f206e4f-9f45-4459-cd80-3eb358104520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 4s 21ms/step - loss: 0.3232 - accuracy: 0.8664\n",
            "Test Loss: 0.3232058882713318\n",
            "Test Accuracy: 0.8664000034332275\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(test_dataset)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-13T03:57:34.579884Z",
          "iopub.status.busy": "2021-01-13T03:57:34.579199Z",
          "iopub.status.idle": "2021-01-13T03:57:36.666160Z",
          "shell.execute_reply": "2021-01-13T03:57:36.665530Z"
        },
        "id": "ZXgfQSgRW6zU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8450809c-4b85-4041-944e-c879af8bb19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "the film was really bad and i am very disappointed\n",
            "Sentiment:  0.25\n",
            "\n",
            "The film was very funny entertaining and good we had a great time . brilliant film\n",
            "Sentiment:  0.93\n",
            "\n",
            "this film was just brilliant\n",
            "Sentiment:  0.39\n",
            "\n",
            "This movie has been a disaster\n",
            "Sentiment:  0.26\n",
            "\n",
            "the movie is not bad\n",
            "Sentiment:  0.07\n"
          ]
        }
      ],
      "source": [
        "reviews = ['the film was really bad and i am very disappointed',\n",
        "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "           'this film was just brilliant',\n",
        "          'This movie has been a disaster',\n",
        "           'the movie is not bad']\n",
        "predictions = model.predict(np.array(reviews))\n",
        "\n",
        "for review, pred in zip(reviews, predictions.flatten()):\n",
        "    print()\n",
        "    print(review)\n",
        "    print('Sentiment: ', np.round(pred, 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pNm0f_3Gza14"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}