{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtGdNOkEPZ1"
      },
      "source": [
        "#1 - Tuber√≠a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-UzAlMP03u"
      },
      "source": [
        "##1.0 - Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyJoyUXY3D3B",
        "outputId": "dabf9d0b-650b-4654-91d4-74ef44287c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji_extractor\n",
            "  Downloading emoji_extractor-1.0.20.tar.gz (34 kB)\n",
            "Building wheels for collected packages: emoji-extractor\n",
            "  Building wheel for emoji-extractor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji-extractor: filename=emoji_extractor-1.0.20-py3-none-any.whl size=63580 sha256=9c06918564708cd73d7c534ca7a7fa967c7e00a477ce4d2eeae4d1d748f03d4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/3a/10/4d60270bd4c7b1569ee392d611fbcbefc2af7d7245163913e3\n",
            "Successfully built emoji-extractor\n",
            "Installing collected packages: emoji-extractor\n",
            "Successfully installed emoji-extractor-1.0.20\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=8467dd4372a82473882116eca8d8bda62ae064f7a7996314170cee923e94824e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.7 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
            "--2022-09-06 21:40:57--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409871727 (391M) [application/x-gzip]\n",
            "Saving to: ‚Äòpytorch_weights.tar.gz‚Äô\n",
            "\n",
            "pytorch_weights.tar 100%[===================>] 390.88M  8.25MB/s    in 50s     \n",
            "\n",
            "2022-09-06 21:41:48 (7.81 MB/s) - ‚Äòpytorch_weights.tar.gz‚Äô saved [409871727/409871727]\n",
            "\n",
            "--2022-09-06 21:41:49--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 242120 (236K) [text/plain]\n",
            "Saving to: ‚Äòvocab.txt‚Äô\n",
            "\n",
            "vocab.txt           100%[===================>] 236.45K   403KB/s    in 0.6s    \n",
            "\n",
            "2022-09-06 21:41:50 (403 KB/s) - ‚Äòvocab.txt‚Äô saved [242120/242120]\n",
            "\n",
            "--2022-09-06 21:41:50--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 313 [application/json]\n",
            "Saving to: ‚Äòconfig.json‚Äô\n",
            "\n",
            "config.json         100%[===================>]     313  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-06 21:41:51 (48.9 MB/s) - ‚Äòconfig.json‚Äô saved [313/313]\n",
            "\n",
            "pytorch/\n",
            "pytorch/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import re\n",
        "import datetime as dt\n",
        "# Instalaci√≥n de la API\n",
        "#!pip install git+https://github.com/tweepy/tweepy.git\n",
        "#import tweepy\n",
        "from tqdm import tqdm\n",
        "!pip install emoji_extractor\n",
        "from emoji_extractor.extract import Extractor\n",
        "!pip install clean-text\n",
        "from cleantext import clean\n",
        "!pip install transformers\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
        "!tar -xzvf pytorch_weights.tar.gz\n",
        "!mv config.json pytorch/.\n",
        "!mv vocab.txt pytorch/.\n",
        "##import torch\n",
        "from transformers import BertForMaskedLM, BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z34TO-O1FeJH"
      },
      "source": [
        "##1.3 - Tweets\n",
        "Descarga de las publicaciones (parte de la extracci√≥n) y transformaci√≥n.\n",
        "\n",
        "Generaci√≥n del marco de datos o dataset. Se incluyen los siguientes campos: nombre, usuario, ubicaci√≥n (si lo tiene), descripci√≥n del usuario, verificaci√≥n de la cuenta, fecha, texto, hashtags, y fuente (dispositivo) del tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nZo59YrSZFZs",
        "outputId": "51e7ba88-0bcb-48f6-8828-8d3181ecae33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b10efac3-c652-427c-a2ff-e83146e27d8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VALORACIONES</th>\n",
              "      <th>Valor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>accentos</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mayusculas</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sign_punt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>emojis</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abrev</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>extranjerismos</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>repet_termin</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>beto</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b10efac3-c652-427c-a2ff-e83146e27d8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b10efac3-c652-427c-a2ff-e83146e27d8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b10efac3-c652-427c-a2ff-e83146e27d8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     VALORACIONES  Valor\n",
              "0        accentos      1\n",
              "1      mayusculas      1\n",
              "2       sign_punt      1\n",
              "3          emojis     -1\n",
              "4           abrev     -1\n",
              "5  extranjerismos     -1\n",
              "6    repet_termin     -1\n",
              "7            beto      0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_val = {'accentos':1, 'mayusculas':1, 'sign_punt':1, 'emojis':-1, 'abrev':-1, 'extranjerismos': -1, 'repet_termin':-1, 'beto':0 }\n",
        "df_val = pd.DataFrame([[key, dict_val[key]] for key in dict_val.keys()], columns=['VALORACIONES', 'Valor'])\n",
        "df_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq4VzYma_gYj"
      },
      "source": [
        "---\n",
        "# 2 - Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFNVeYX212l7"
      },
      "source": [
        "### 2.1 - Preprocesado\n",
        "Eliminaci√≥n de las palabras sin sentido o *stop words* y agrupaci√≥n de emojis al final de la cadena.\n",
        "\n",
        "**¬°OJO! Aquellas que contengan acentos o est√©n mal escritas, se quedar√°n.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VjbfmqGDUERm",
        "outputId": "38e3d9ec-df7a-4b13-8758-9f84db5fbacf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-da80ab09-1db2-4f39-b22a-bc690a6d0b70\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NA_1</td>\n",
              "      <td>Esta prueba consiste en quedarnos √∫nicamente c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NA_2</td>\n",
              "      <td>En un lugar de la Mancha, de cuyo nombre no qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NA_3</td>\n",
              "      <td>√Årbol de la familia de las eben√°ceas, de diez ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NA_4</td>\n",
              "      <td>Mi t√≠o tiene un apartamento con mucha luz y po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NA_5</td>\n",
              "      <td>El resto della conclu√≠an sayo de velarte, calz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NA_6</td>\n",
              "      <td>Juego de mesa entre dos personas que se practi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NA_7</td>\n",
              "      <td>Red inform√°tica mundial, descentralizada, form...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NA_8</td>\n",
              "      <td>Instrumento astron√≥mico usado antiguamente par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NA_9</td>\n",
              "      <td>Cuando los d√≠as pasan y las horas transcurren ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NA_10</td>\n",
              "      <td>Arbusto cultivado originario de Asia, de la fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NB_1</td>\n",
              "      <td>coger el resto... Prueba, lol . Pedro, nadies ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NB_2</td>\n",
              "      <td>Es una obra de arte porque el arte es asi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NB_3</td>\n",
              "      <td>La prueba ya con m√°s cositas como üòçüòçüò∫üò∫üëèüëèüëè. \\n ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NB_4</td>\n",
              "      <td>tkm, estao ha de salir mal sino no sirve üòçüòçüòç.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NB_5</td>\n",
              "      <td>El arbol tene frutas buenasüò∫, plz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NB_6</td>\n",
              "      <td>mi amiga se fue corriendo a su casa para la in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NB_7</td>\n",
              "      <td>Estoy empezando a estudiar Franc√©s rt, omg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NB_8</td>\n",
              "      <td>Las nuevas generaciones crecen muy rapido, tan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NB_9</td>\n",
              "      <td>la casa de mis padres es grande y tiene una pi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NB_10</td>\n",
              "      <td>Los anacardos de volor verde aun no estan list...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da80ab09-1db2-4f39-b22a-bc690a6d0b70')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da80ab09-1db2-4f39-b22a-bc690a6d0b70 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da80ab09-1db2-4f39-b22a-bc690a6d0b70');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     name                                               text\n",
              "0    NA_1  Esta prueba consiste en quedarnos √∫nicamente c...\n",
              "1    NA_2  En un lugar de la Mancha, de cuyo nombre no qu...\n",
              "2    NA_3  √Årbol de la familia de las eben√°ceas, de diez ...\n",
              "3    NA_4  Mi t√≠o tiene un apartamento con mucha luz y po...\n",
              "4    NA_5  El resto della conclu√≠an sayo de velarte, calz...\n",
              "5    NA_6  Juego de mesa entre dos personas que se practi...\n",
              "6    NA_7  Red inform√°tica mundial, descentralizada, form...\n",
              "7    NA_8  Instrumento astron√≥mico usado antiguamente par...\n",
              "8    NA_9  Cuando los d√≠as pasan y las horas transcurren ...\n",
              "9   NA_10  Arbusto cultivado originario de Asia, de la fa...\n",
              "10   NB_1  coger el resto... Prueba, lol . Pedro, nadies ...\n",
              "11   NB_2          Es una obra de arte porque el arte es asi\n",
              "12   NB_3  La prueba ya con m√°s cositas como üòçüòçüò∫üò∫üëèüëèüëè. \\n ...\n",
              "13   NB_4      tkm, estao ha de salir mal sino no sirve üòçüòçüòç.\n",
              "14   NB_5                  El arbol tene frutas buenasüò∫, plz\n",
              "15   NB_6  mi amiga se fue corriendo a su casa para la in...\n",
              "16   NB_7         Estoy empezando a estudiar Franc√©s rt, omg\n",
              "17   NB_8  Las nuevas generaciones crecen muy rapido, tan...\n",
              "18   NB_9  la casa de mis padres es grande y tiene una pi...\n",
              "19  NB_10  Los anacardos de volor verde aun no estan list..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Textos de prueba\n",
        "NA_1 = \"Esta prueba consiste en quedarnos √∫nicamente con las palabras y eliminar aquellas sin significado. \"\n",
        "NA_2 = 'En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv√≠a un hidalgo de los de lanza en astillero, adarga antigua, roc√≠n flaco y galgo corredor. '\n",
        "NA_3 = '√Årbol de la familia de las eben√°ceas, de diez a doce metros de altura y de copa ancha.'\n",
        "NA_4 = 'Mi t√≠o tiene un apartamento con mucha luz y por la ventana se puede ver el mar.'\n",
        "NA_5 = 'El resto della conclu√≠an sayo de velarte, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo, y los d√≠as de entresemana se honraba con su vellor√≠ de lo m√°s fino. Ten√≠a en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que as√≠ ensillaba el roc√≠n como tomaba la podadera.'\n",
        "NA_6 = 'Juego de mesa entre dos personas que se practica sobre un damero en el que se disponen las 16 piezas de cada jugador, desiguales en importancia y valor, que se desplazan y comen las del contrario seg√∫n ciertas reglas.'\n",
        "NA_7 = 'Red inform√°tica mundial, descentralizada, formada por la conexi√≥n directa entre computadoras mediante un protocolo especial de comunicaci√≥n.'\n",
        "NA_8 = 'Instrumento astron√≥mico usado antiguamente para determinar la posici√≥n de los astros.'\n",
        "NA_9 = 'Cuando los d√≠as pasan y las horas transcurren te das cuenta de que la soledad no es una buena amiga.'\n",
        "NA_10 = 'Arbusto cultivado originario de Asia, de la familia de las rut√°ceas, de unos dos metros de altura, ramoso, con hojas casi persistentes, opuestas, aovadas, lisas y lustrosas, flores peque√±as en racimo, blancas y olorosas y por frutos bayas rojas, redondas y del tama√±o de un guisante üò∫ .'\n",
        "\n",
        "# Textos de prueba\n",
        "NB_1 = \"coger el resto... Prueba, lol . Pedro, nadies hindu tkm \"\n",
        "NB_2 = 'Es una obra de arte porque el arte es asi'\n",
        "NB_3 = 'La prueba ya con m√°s cositas como üòçüòçüò∫üò∫üëèüëèüëè. \\n A ver si funciona üòçüòçüò∫üò∫, LOL'\n",
        "NB_4 = 'tkm, estao ha de salir mal sino no sirve üòçüòçüòç.'\n",
        "NB_5 = 'El arbol tene frutas buenasüò∫, plz'\n",
        "NB_6 = 'mi amiga se fue corriendo a su casa para la inaguraci√≥n'\n",
        "NB_7 = 'Estoy empezando a estudiar Franc√©s rt, omg'\n",
        "NB_8 = 'Las nuevas generaciones crecen muy rapido, tan rapido como nosotros.'\n",
        "NB_9 = 'la casa de mis padres es grande y tiene una picina que comprastes'\n",
        "NB_10 = 'Los anacardos de volor verde aun no estan listos para ser recogidos'\n",
        "\n",
        "\n",
        "\n",
        "# Creaci√≥n del DataFrame para probar, no ser√° necesario porque vendr√° de la tuber√≠a\n",
        "dicc = {'name': ['NA_1', 'NA_2', 'NA_3', 'NA_4', 'NA_5', 'NA_6', 'NA_7', 'NA_8', 'NA_9', 'NA_10', 'NB_1', 'NB_2', 'NB_3', 'NB_4', 'NB_5', 'NB_6', 'NB_7', 'NB_8', 'NB_9', 'NB_10'], \n",
        "        'text': [NA_1, NA_2, NA_3, NA_4, NA_5, NA_6, NA_7, NA_8, NA_9, NA_10, NB_1, NB_2, NB_3, NB_4, NB_5, NB_6, NB_7, NB_8, NB_9, NB_10]}\n",
        "df_demo = pd.DataFrame(dicc)\n",
        "df_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wD3TGDUaHdWv",
        "outputId": "06ab7a65-c168-411c-ead8-6263449b09fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# Diccionario espa√±ol\n",
        "stops = set(stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sW9Cq2CKgccT",
        "outputId": "e7a3590b-613e-4182-af79-bae4815e1426"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cd668fc5-244e-4e39-bed2-5002f503880b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NA_1</td>\n",
              "      <td>Esta prueba consiste en quedarnos √∫nicamente c...</td>\n",
              "      <td>Esta prueba consiste en quedarnos √∫nicamente c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NA_2</td>\n",
              "      <td>En un lugar de la Mancha, de cuyo nombre no qu...</td>\n",
              "      <td>En un lugar de la Mancha, de cuyo nombre no qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NA_3</td>\n",
              "      <td>√Årbol de la familia de las eben√°ceas, de diez ...</td>\n",
              "      <td>√Årbol de la familia de las eben√°ceas, de diez ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NA_4</td>\n",
              "      <td>Mi t√≠o tiene un apartamento con mucha luz y po...</td>\n",
              "      <td>Mi t√≠o tiene un apartamento con mucha luz y po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NA_5</td>\n",
              "      <td>El resto della conclu√≠an sayo de velarte, calz...</td>\n",
              "      <td>El resto della conclu√≠an sayo de velarte, calz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NA_6</td>\n",
              "      <td>Juego de mesa entre dos personas que se practi...</td>\n",
              "      <td>Juego de mesa entre dos personas que se practi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NA_7</td>\n",
              "      <td>Red inform√°tica mundial, descentralizada, form...</td>\n",
              "      <td>Red inform√°tica mundial, descentralizada, form...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NA_8</td>\n",
              "      <td>Instrumento astron√≥mico usado antiguamente par...</td>\n",
              "      <td>Instrumento astron√≥mico usado antiguamente par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NA_9</td>\n",
              "      <td>Cuando los d√≠as pasan y las horas transcurren ...</td>\n",
              "      <td>Cuando los d√≠as pasan y las horas transcurren ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NA_10</td>\n",
              "      <td>Arbusto cultivado originario de Asia, de la fa...</td>\n",
              "      <td>Arbusto cultivado originario de Asia, de la fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NB_1</td>\n",
              "      <td>coger el resto... Prueba, lol . Pedro, nadies ...</td>\n",
              "      <td>coger el resto... Prueba, lol . Pedro, nadies ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NB_2</td>\n",
              "      <td>Es una obra de arte porque el arte es asi</td>\n",
              "      <td>Es una obra de arte porque el arte es asi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NB_3</td>\n",
              "      <td>La prueba ya con m√°s cositas como üòçüòçüò∫üò∫üëèüëèüëè. \\n ...</td>\n",
              "      <td>La prueba ya con m√°s cositas como . A ver si f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NB_4</td>\n",
              "      <td>tkm, estao ha de salir mal sino no sirve üòçüòçüòç.</td>\n",
              "      <td>tkm, estao ha de salir mal sino no sirve . üòç</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NB_5</td>\n",
              "      <td>El arbol tene frutas buenasüò∫, plz</td>\n",
              "      <td>El arbol tene frutas buenas, plz üò∫</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NB_6</td>\n",
              "      <td>mi amiga se fue corriendo a su casa para la in...</td>\n",
              "      <td>mi amiga se fue corriendo a su casa para la in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NB_7</td>\n",
              "      <td>Estoy empezando a estudiar Franc√©s rt, omg</td>\n",
              "      <td>Estoy empezando a estudiar Franc√©s rt, omg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NB_8</td>\n",
              "      <td>Las nuevas generaciones crecen muy rapido, tan...</td>\n",
              "      <td>Las nuevas generaciones crecen muy rapido, tan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NB_9</td>\n",
              "      <td>la casa de mis padres es grande y tiene una pi...</td>\n",
              "      <td>la casa de mis padres es grande y tiene una pi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NB_10</td>\n",
              "      <td>Los anacardos de volor verde aun no estan list...</td>\n",
              "      <td>Los anacardos de volor verde aun no estan list...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd668fc5-244e-4e39-bed2-5002f503880b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd668fc5-244e-4e39-bed2-5002f503880b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd668fc5-244e-4e39-bed2-5002f503880b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     name                                               text  \\\n",
              "0    NA_1  Esta prueba consiste en quedarnos √∫nicamente c...   \n",
              "1    NA_2  En un lugar de la Mancha, de cuyo nombre no qu...   \n",
              "2    NA_3  √Årbol de la familia de las eben√°ceas, de diez ...   \n",
              "3    NA_4  Mi t√≠o tiene un apartamento con mucha luz y po...   \n",
              "4    NA_5  El resto della conclu√≠an sayo de velarte, calz...   \n",
              "5    NA_6  Juego de mesa entre dos personas que se practi...   \n",
              "6    NA_7  Red inform√°tica mundial, descentralizada, form...   \n",
              "7    NA_8  Instrumento astron√≥mico usado antiguamente par...   \n",
              "8    NA_9  Cuando los d√≠as pasan y las horas transcurren ...   \n",
              "9   NA_10  Arbusto cultivado originario de Asia, de la fa...   \n",
              "10   NB_1  coger el resto... Prueba, lol . Pedro, nadies ...   \n",
              "11   NB_2          Es una obra de arte porque el arte es asi   \n",
              "12   NB_3  La prueba ya con m√°s cositas como üòçüòçüò∫üò∫üëèüëèüëè. \\n ...   \n",
              "13   NB_4      tkm, estao ha de salir mal sino no sirve üòçüòçüòç.   \n",
              "14   NB_5                  El arbol tene frutas buenasüò∫, plz   \n",
              "15   NB_6  mi amiga se fue corriendo a su casa para la in...   \n",
              "16   NB_7         Estoy empezando a estudiar Franc√©s rt, omg   \n",
              "17   NB_8  Las nuevas generaciones crecen muy rapido, tan...   \n",
              "18   NB_9  la casa de mis padres es grande y tiene una pi...   \n",
              "19  NB_10  Los anacardos de volor verde aun no estan list...   \n",
              "\n",
              "                                           text_clean  \n",
              "0   Esta prueba consiste en quedarnos √∫nicamente c...  \n",
              "1   En un lugar de la Mancha, de cuyo nombre no qu...  \n",
              "2   √Årbol de la familia de las eben√°ceas, de diez ...  \n",
              "3   Mi t√≠o tiene un apartamento con mucha luz y po...  \n",
              "4   El resto della conclu√≠an sayo de velarte, calz...  \n",
              "5   Juego de mesa entre dos personas que se practi...  \n",
              "6   Red inform√°tica mundial, descentralizada, form...  \n",
              "7   Instrumento astron√≥mico usado antiguamente par...  \n",
              "8   Cuando los d√≠as pasan y las horas transcurren ...  \n",
              "9   Arbusto cultivado originario de Asia, de la fa...  \n",
              "10  coger el resto... Prueba, lol . Pedro, nadies ...  \n",
              "11         Es una obra de arte porque el arte es asi   \n",
              "12  La prueba ya con m√°s cositas como . A ver si f...  \n",
              "13       tkm, estao ha de salir mal sino no sirve . üòç  \n",
              "14                 El arbol tene frutas buenas, plz üò∫  \n",
              "15  mi amiga se fue corriendo a su casa para la in...  \n",
              "16        Estoy empezando a estudiar Franc√©s rt, omg   \n",
              "17  Las nuevas generaciones crecen muy rapido, tan...  \n",
              "18  la casa de mis padres es grande y tiene una pi...  \n",
              "19  Los anacardos de volor verde aun no estan list...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Limpieza de datos\n",
        "# Movemos emojis al final y removemos espacios de m√°s\n",
        "def clean_df(df):\n",
        "\n",
        "  extract = Extractor()\n",
        "  aux = df['text'].apply(lambda x: \" \".join(extract.count_emoji(x).keys() ))\n",
        "  aux2 = df['text'].apply(lambda x: clean( x, \n",
        "                                              lower=False,no_emoji=True,\n",
        "                                              no_line_breaks=True,\n",
        "                                              fix_unicode=False,\n",
        "                                              to_ascii=False\n",
        "                                          )\n",
        "                )\n",
        "  aux = aux2+\" \"+aux\n",
        "\n",
        "  df['text_clean'] = aux\n",
        "  return df\n",
        "\n",
        "df_demo = clean_df(df_demo)\n",
        "df_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7y935fOXHDUk"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords_(tweet):\n",
        "  df_demo['stop_words'] = df_demo['text_clean']\\\n",
        "                        .apply(lambda x: ' '.join([word for word in x.split() if word not in (stops)]))\n",
        "  return(df_demo)\n",
        "\n",
        "df_demo = remove_stopwords_(df_demo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hRaekqY9eFb"
      },
      "source": [
        "## 2.2 - Valoraciones\n",
        "Cada una de las pr√≥ximas iteraciones representa una columna con una puntuaci√≥n por cada usuario o fila. \n",
        "\n",
        "El sentido funcional de las valoraciones est√° basado de forma relativa, es decir, una proporci√≥n (modelo no supervisado). \n",
        "\n",
        "El sentido operacional es similar al de un modelo de one-hot, donde finalmente, una fila sumar√° la puntuaci√≥n obtenida en cada columna o iteraci√≥n.\n",
        "\n",
        "El resultado ser√° una agrupaci√≥n por puntuaciones obtenidas, pudi√©ndose agrupar tanto a nivel vertical como horizontal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t_1eDed94wr"
      },
      "source": [
        "### 2.2.1 - May√∫sculas\n",
        "Numerador: n¬∫ de may√∫sculas.\n",
        "\n",
        "Denominador: n¬∫ de palabras (sin stop_words, igual para el resto)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_o5ccTsWA8BA"
      },
      "outputs": [],
      "source": [
        "def get_capital_letters_(tweet):\n",
        "\tresult = re.findall(r'[A-Z]',tweet)\n",
        "\tstring = \"\".join(result)\n",
        "\treturn list(string)\n",
        "# Nueva columna pasando la funci√≥n por cada una de las filas\t\n",
        "df_demo['capital_letters'] = df_demo['stop_words'].apply(lambda x : len(get_capital_letters_(x))/len(x.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5McOHwG_rvu"
      },
      "source": [
        "### 2.2.2 - Acentos\n",
        "Numerador: n¬∫ de palabras con acentos.\n",
        "\n",
        "Denominador: n¬∫ de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pEqqkqpE08Po"
      },
      "outputs": [],
      "source": [
        "def find_accent_words(tweet):\n",
        "  acentos = [\"√°√©√≠√≥√∫√º√Å√â√ç√ì√ö√ú\"]\n",
        "  words = np.array([])\n",
        "  for pat in acentos[0]:\n",
        "    aux = \"\\w*[\"+pat+\"]\\w*\"\n",
        "    aux = re.findall(aux, tweet)\n",
        "    if len(aux) > 0:\n",
        "      words = np.append(words, aux)\n",
        "  return(len(words)/len(tweet.split()))\n",
        "\n",
        "df_demo['accents'] = df_demo['stop_words'].apply(lambda x: find_accent_words(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb1jtFnXvh3"
      },
      "source": [
        "### 2.2.3 - Signos de Puntuaci√≥n\n",
        "Numerador: n¬∫ de signos.\n",
        "\n",
        "Denominador: n¬∫ de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BIUBWMpIPF2M"
      },
      "outputs": [],
      "source": [
        "def get_punctuations_marks_(tweet):\n",
        "\tresult = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*',\n",
        "\t\t\t\t\t\ttweet)\n",
        "\tstring = \"\".join(result)\n",
        "\treturn list(string)\n",
        "# Nueva columna pasando la funci√≥n por cada una de las filas\t\n",
        "df_demo['punctuation_marks'] = df_demo['stop_words'].apply(lambda x : len(get_punctuations_marks_(x))/len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlIu8PFjh_2z"
      },
      "source": [
        "### 2.2.4 - Longitud de Palabra\n",
        "Numerador: suma de los caracteres de todas las palabras.\n",
        "\n",
        "Denominador: n¬∫ de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e_sYw3AhCveu"
      },
      "outputs": [],
      "source": [
        "# Longitud de cada palabra -> String de n√∫meros\n",
        "def word_length(tweet):\n",
        "  aux = [len(x) for x in tweet.split()]\n",
        "  return aux\n",
        "\n",
        "# Suma de los n√∫meros de cada string para luego relativizarlo entre el n√∫mero de palabras\n",
        "def sum_numbers(numbers):\n",
        "     if len(numbers) == 0:\n",
        "         return 0\n",
        "     return numbers[0] + sum_numbers(numbers[1:])\n",
        "\n",
        "df_demo['word_length'] = df_demo['stop_words'].apply(lambda x: sum_numbers(word_length(x))/len(x.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTAx-jI2BrNK"
      },
      "source": [
        "### 2.2.5 - Emojis\n",
        "Numerador: n¬∞ de emojis √∫nicos.\n",
        "\n",
        "Denominador: n¬∞ de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LeQy1Bdqh-hI"
      },
      "outputs": [],
      "source": [
        "def detect_emoji(tweet):\n",
        "  extract = Extractor()\n",
        "  return len(extract.count_emoji(tweet).keys())/len(tweet.split())\n",
        "\n",
        "df_demo['emoji'] = df_demo['stop_words'].apply(lambda x: detect_emoji(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIVF4OuyBzRi"
      },
      "source": [
        "### 2.2.6 - Abreviaciones\n",
        "Numerador: n¬∫ de abreviaciones.\n",
        "\n",
        "Denominador: n¬∫ de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FhkL40o_6V06"
      },
      "outputs": [],
      "source": [
        "def get_abbreviation_(tweet):\n",
        "  abbr = ['rofl','pa','rl','lol','ptm','tqm','tkm','bb',\n",
        "         'fb','gpi','yt','tl','pls','oc','wtf','bc','bfn',\n",
        "         'dm','ftf','icymi','imho','irl','lmlt','np','oh',\n",
        "         'plz','qotd','rt','prt','rtrl','tmi','ty','tt','tl',\n",
        "         'cc','fa','fyi','+1','afk','brb','gtg','ht','hth', 'imo','lmao']\n",
        "  words = np.array([])\n",
        "  for pat in abbr[:]:\n",
        "    aux = \" \"+pat+\" \"\n",
        "    aux = re.findall(aux, tweet)\n",
        "    if len(aux) > 0:\n",
        "      words = np.append(words, aux)\n",
        "  return (len(words)/len(tweet.split()))\n",
        "  \n",
        "df_demo['abbreviation'] = df_demo['stop_words'].apply(lambda x: get_abbreviation_(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwRkfxyJCGwi"
      },
      "source": [
        "### 2.2.7 - Barbarismos\n",
        "Numerador: n¬∫ de extranjerismos.\n",
        "\n",
        "Denominador: n¬∫ de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Od-tOHUQRrqm"
      },
      "outputs": [],
      "source": [
        "def get_barbarism_(tweet):\n",
        "  barbar = ['comprastes','guevo','inaguraci√≥n','nadies','picsa','custi√≥n',\n",
        "          'interperie','fuistes','ambos dos','jrito','hebreo','vertir',\n",
        "          'hind√∫','trompezar','adici√≥n','exepto','lego','l√≠bido','hubiero',\n",
        "          'idiosincracia','beneficiencia','visicitud','suscinto','af√©rrimo',\n",
        "          'exc√©ptico','convalescencia','discrecci√≥n','esplanada','innundaci√≥n',\n",
        "          'fideligno','fregaplatos','inexcrutable','mis√≥geno','prevadicaci√≥n',\n",
        "          'subrealista','sujecci√≥n','transtornado','exalar','exhuberante',\n",
        "          'exumar','exausto','exibir']\n",
        "  words = np.array([])\n",
        "  for pat in barbar[:]:\n",
        "    aux = \"\\w*\"+pat+\"\\w*\"\n",
        "    aux = re.findall(aux, tweet)\n",
        "    if len(aux) > 0:\n",
        "      words = np.append(words, aux)\n",
        "  return (len(words)/len(tweet.split()))\n",
        "  \n",
        "df_demo['barbarism'] = df_demo['stop_words'].apply(lambda x: get_barbarism_(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_8ubRKYCRRe"
      },
      "source": [
        "### 2.2.8 - Repetici√≥n de T√©rminos\n",
        "Numerador: n¬∞ de palabras √∫nicas.\n",
        "\n",
        "Denominador: n¬∞ de palabras totales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nTg-k3m0J3oT"
      },
      "outputs": [],
      "source": [
        "def repeated_words(tweet):\n",
        "  aux = tweet.split()\n",
        "  aux_unique = np.unique(aux)\n",
        "  return(1 - len(aux_unique)/len(aux))\n",
        "  \n",
        "df_demo['repeticion'] = df_demo['stop_words'].apply(lambda x: repeated_words(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS9yM7N8CbiC"
      },
      "source": [
        "### 2.2.9 - BETO\n",
        "*Comparaci√≥n con respuestas del BETO*.\n",
        "\n",
        "Numerador: N¬∞ de palabras coincidentes con las primeras 10 propuestas del BETO.\n",
        "\n",
        "Denominador: N¬∞ de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "shyD_g9opfPg",
        "outputId": "397dadc8-9c59-4e1e-c935-aa88b1c95676"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a32a6f208454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# beto_pred(\"¬øhola como estas? Espero no del todo mal. A√∫n no recibo la postal. Que prometiste el d√≠a en que te fuiste.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdf_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BETO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeto_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-a32a6f208454>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# beto_pred(\"¬øhola como estas? Espero no del todo mal. A√∫n no recibo la postal. Que prometiste el d√≠a en que te fuiste.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdf_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BETO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeto_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-a32a6f208454>\u001b[0m in \u001b[0;36mbeto_pred\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Predicci√≥n del Tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "def beto_pred(tweet):\n",
        "  split_tweet = re.findall(r\"[\\w']+|[.,¬°!¬ø?;:]\", tweet)\n",
        "  c = 0\n",
        "\n",
        "  \n",
        "  for i in range(len(split_tweet)):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
        "    model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
        "    e = model.eval()\n",
        "    # Ciclo por cada palabra o signo de interrogaci√≥n del tweet\n",
        "    text = split_tweet.copy()\n",
        "    # Copia de la palabra a buscar.\n",
        "    real_word = text[i]\n",
        "    # Enmascarado de la palabra a buscar\n",
        "    text[i] = \"[MASK]\"\n",
        "    text = \"[CLS] \"+\" \".join(text)+\" [SEP]\"\n",
        "    \n",
        "    # Tokenizaci√≥n del Tweet\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "    # Predicci√≥n del Tweet\n",
        "    predictions = model(tokens_tensor)[0]\n",
        "\n",
        "    idxs = torch.argsort(predictions[0, i+1], descending=True)\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:2])\n",
        "\n",
        "    # print(text)\n",
        "    # print(predicted_token)\n",
        "    c = c + (real_word in predicted_token)\n",
        "  return c/len(split_tweet)\n",
        "    \n",
        "\n",
        "# beto_pred(\"¬øhola como estas? Espero no del todo mal. A√∫n no recibo la postal. Que prometiste el d√≠a en que te fuiste.\")\n",
        "\n",
        "df_demo['BETO'] = df_demo['text_clean'].apply(lambda x: beto_pred(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4-FZ6KsMbQPE"
      },
      "outputs": [],
      "source": [
        "df_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm6fuQUjAMMa"
      },
      "source": [
        "---\n",
        "## 2.3 - Segmentaci√≥n / Clustering\n",
        "Agrupaci√≥n de las valoraciones por segmentos o clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n5b6OXB0APxa"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jMyYQWeM8wW8"
      },
      "outputs": [],
      "source": [
        "df_val = df_demo.iloc[:,4:] # Selecionar filas de valoraciones\n",
        "array_val = df_val.to_numpy()# Convertir a array\n",
        "scaler = StandardScaler()#Escalamos los valores de l'array\n",
        "scaled_val = scaler.fit_transform(array_val)\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uk6gXDCpBeqR"
      },
      "outputs": [],
      "source": [
        "# K-means con 2 clusters(mayor/menot nivel de educaci√≥n)\n",
        "kmeans = KMeans(\n",
        "  init=\"random\",\n",
        "  n_clusters=2,\n",
        "  n_init=10,\n",
        "  max_iter=300,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "kmeans.fit(scaled_val)\n",
        "\n",
        "# Categoria de cada tweet en el cluster\n",
        "kmeans_label = kmeans.labels_[:]\n",
        "label_val = list(kmeans_label)\n",
        "# Adjuntar valores (label) al df\n",
        "df_demo.loc[:,'Label'] = (label_val)\n",
        "df_final = df_demo.iloc[:,[0,4,5,6,7,8,9,10,11]]\n",
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XAiK-nTLB54V"
      },
      "outputs": [],
      "source": [
        "# Localizaci√≥n de los centroides\n",
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEtP16sLIDlE"
      },
      "source": [
        "###PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jpc9nA07HAEs"
      },
      "outputs": [],
      "source": [
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduc_val = pca.fit_transform(scaled_val)\n",
        "reduc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9PBVZdseIPqx"
      },
      "outputs": [],
      "source": [
        "pca_reduc_df = pd.DataFrame(data=reduc_val, columns = ['Componente_1', 'Componente_2'])\n",
        "pca_label_df = pd.concat([pca_reduc_df, df_demo[['Label']]], axis=1)\n",
        "\n",
        "pca_label_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BnxCsJMEOtfM"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (6,6))\n",
        "\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.set_xlabel('Componente 1', fontsize = 15)\n",
        "ax.set_ylabel('Componente 2', fontsize = 15)\n",
        "ax.set_title('Componentes Principales', fontsize = 20)\n",
        "\n",
        "color_theme = np.array([\"blue\", \"green\", \"orange\"])\n",
        "ax.scatter(x = pca_reduc_df.Componente_1, y = pca_label_df.Componente_2,\n",
        "           c=color_theme[pca_label_df.Label], s=10)\n",
        "#plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='black', s=20)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}